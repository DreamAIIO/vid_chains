{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from vid_chains.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def load_obj_model(\n",
    "        yolo_name:str = \"yolov8n.pt\",\n",
    "        sam_name:str = \"sam_b.pt\", \n",
    "        task:str = \"detect\"\n",
    "    ):\n",
    "    if task == \"detect\":\n",
    "        return YOLO(yolo_name)\n",
    "    elif task == \"segment\":\n",
    "        return SAM(sam_name)\n",
    "    print(f\"Model does not exist for the following task: {task}. Please select one of the following tasks: detect or segment\")\n",
    "    return\n",
    "\n",
    "def detect_objects(\n",
    "        model,\n",
    "        image:Union[np.ndarray, str],\n",
    "        stream:bool = True,\n",
    "        task:str = \"detect\",\n",
    "        conf:float = 0.25,\n",
    "        iou:float = 0.7,\n",
    "        augment:bool = False,\n",
    "        imgsz:int = 640, \n",
    "        names:list = None, \n",
    "        exclude:list = None,   \n",
    "        return_only_boxes:bool = True, \n",
    "        points:list = None, \n",
    "        labels:list = None, \n",
    "        bboxes:list = None\n",
    "    ):\n",
    "    results = None\n",
    "    if task == \"detect\": # Using the yolo model\n",
    "        classes_dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "        classes = list(classes_dict.keys())\n",
    "        if names:\n",
    "            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name.lower())] for name in names]\n",
    "        if exclude:\n",
    "            exclude = [e.lower() for e in exclude]\n",
    "            names = [v for v in list(classes_dict.values()) if not v in exclude]\n",
    "            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name)] for name in names]\n",
    "        # print(classes)\n",
    "        results = model(image, stream=stream, classes=classes, conf=conf, iou=iou, augment=augment, imgsz=imgsz)\n",
    "        if return_only_boxes:\n",
    "            return [{\"boxes\": r.boxes.data.detach().cpu().tolist()} for r in results]\n",
    "    elif task == \"segment\": # Using the SAM model..\n",
    "        results = model(image, stream = stream, bboxes = bboxes, points = points, labels = labels)\n",
    "    detections = None\n",
    "    if results:\n",
    "        for result in results:\n",
    "            detections = sv.Detections.from_ultralytics(result)\n",
    "        return detections\n",
    "    print(f\"Can not process the following task: {task}. Please select one of the following tasks: detect or segment\")\n",
    "    return\n",
    "\n",
    "    \n",
    "def annotateImage(\n",
    "        image:np.ndarray, \n",
    "        results, \n",
    "        draw_bbox:bool = True,\n",
    "        draw_mask:bool = False, \n",
    "        label:bool = True,\n",
    "        conf_thresh:float = 0.0, \n",
    "        names:list = None, \n",
    "        exclude:list = None, \n",
    "        area_thresh:int = 0\n",
    "    ):\n",
    "    labels = None\n",
    "    detections=results\n",
    "    if label:\n",
    "        classes_dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "        classes = list(classes_dict.keys())\n",
    "        if names:\n",
    "            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name.lower())] for name in names]\n",
    "        if exclude:\n",
    "            exclude = [e.lower() for e in exclude]\n",
    "            names = [v for v in list(classes_dict.values()) if not v in exclude]\n",
    "            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name)] for name in names]\n",
    "        detections = detections[detections.confidence > conf_thresh]\n",
    "        detections = detections[np.isin(detections.class_id, classes)]\n",
    "        labels = [\n",
    "            f\"{classes_dict[class_id]} {confidence:.2f}\"\n",
    "            for class_id, confidence\n",
    "            in zip(detections.class_id, detections.confidence)\n",
    "        ]\n",
    "    detections = detections[detections.area > area_thresh]\n",
    "    annotated_image = image.copy()\n",
    "    if draw_bbox:\n",
    "        box_annotator = sv.BoxAnnotator()\n",
    "        annotated_image = box_annotator.annotate(annotated_image, detections, labels=labels)\n",
    "    if draw_mask:\n",
    "        mask_annotator = sv.MaskAnnotator()\n",
    "        annotated_image = mask_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "    return annotated_image\n",
    "\n",
    "def get_width(l):\n",
    "    w = l[2] - l[0]\n",
    "    return w\n",
    "\n",
    "\n",
    "def list_widths(obj):\n",
    "    w = []\n",
    "    for i in range(0, len(obj.get(\"boxes\"))):\n",
    "        l = []\n",
    "        for j in range(0, 6):\n",
    "            l.append(obj.get(\"boxes\")[i][j])\n",
    "        if (\n",
    "            l[5] == 39.0\n",
    "        ):  # very specific test case for bottles so will ignore other objects, will remove this in the future\n",
    "            width = get_width(l)\n",
    "            w.append(width)\n",
    "    return w\n",
    "\n",
    "\n",
    "def centroid(l):\n",
    "    t = []\n",
    "    cx = (l[0] + l[2]) / 2.0\n",
    "    cy = (l[1] + l[3]) / 2.0\n",
    "    t.append(cx)\n",
    "    t.append(cy)\n",
    "    return t\n",
    "\n",
    "\n",
    "def list_centroids(obj):\n",
    "    c = []\n",
    "    for i in range(0, len(obj.get(\"boxes\"))):\n",
    "        l = []\n",
    "        for j in range(0, 4):\n",
    "            l.append(obj.get(\"boxes\")[i][j])\n",
    "        centre = centroid(l)\n",
    "        c.append(centre)\n",
    "    return c\n",
    "\n",
    "\n",
    "def inter_dist(obj):\n",
    "    c = list_centroids(obj)\n",
    "    dis = []\n",
    "    st = []\n",
    "    for i in range(0, len(c)):\n",
    "        for j in range(i + 1, len(c)):\n",
    "            # st.append(\"Distance b/w object \"+str(i)+\" and object \"+str(j))\n",
    "            # st.append(\"D(\"+str(i)+\",\"+str(j)+\")\")\n",
    "            dis.append(math.dist(c[i], c[j]))\n",
    "    # return st,dis\n",
    "    return dis\n",
    "\n",
    "\n",
    "def focal_len_to_px(focal_len, sensor_px):\n",
    "    return round((focal_len / sensor_px) * 1000)\n",
    "\n",
    "\n",
    "def camera_to_obj_dist(focal_length_px, obj, real_width):\n",
    "    widths = list_widths(obj)\n",
    "    dists = []\n",
    "    for w in widths:\n",
    "        distance = (real_width * focal_length_px) / w\n",
    "        dists.append(distance)\n",
    "\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# Extract direction and speed from the selected objects using RAFT (optical flow algorithm)..\n",
    "\n",
    "\n",
    "def get_points(\n",
    "    yolo,\n",
    "    names:list,\n",
    "    img: Union[str, np.ndarray],\n",
    "    annotate: bool = False,\n",
    "    return_img: bool = False,\n",
    "    stream: bool = True,\n",
    "):\n",
    "    detections = detect_objects(model=yolo, image=img, stream=stream, task=\"detect\", names=names, return_only_boxes=False)\n",
    "    points = []\n",
    "    labels = []\n",
    "    boxes = detections.xyxy\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        mid_x = int(x1 + ((x2 - x1) / 2))\n",
    "        mid_y = int(y1 + ((y2 - y1) / 2))\n",
    "        points.append([mid_x, mid_y])\n",
    "        labels.append(1)  #\n",
    "    if annotate:\n",
    "        annotated_image = annotateImage(image=img, results=detections, draw_bbox=True, draw_mask=False)\n",
    "    if return_img:\n",
    "        return annotated_image, boxes, points, labels\n",
    "    return boxes, points, labels\n",
    "\n",
    "def draw_arrow(img: np.ndarray, mean_u, mean_v, points):\n",
    "    h_rat = 10\n",
    "    w_rat = 10\n",
    "    image_arr = cv2.arrowedLine(\n",
    "        img=img,\n",
    "        pt1=(points[0], points[1]),\n",
    "        pt2=(points[0] + int(mean_u) * w_rat, points[1] + int(mean_v) * h_rat),\n",
    "        color=(0, 0, 255),\n",
    "        thickness=5,\n",
    "        line_type=8,\n",
    "        tipLength=0.5,\n",
    "    )\n",
    "    return image_arr\n",
    "\n",
    "\n",
    "def get_velocity(\n",
    "    img1: Union[str, np.ndarray],\n",
    "    img2: Union[str,np.ndarray],\n",
    "    boxes: list,\n",
    "    res: np.ndarray,\n",
    "    model = None,\n",
    "    save_img: bool = True,\n",
    "    out_dir: str = \"./frames/\",\n",
    "    config_file: str = \"raft_8x2_50k_kitti2015_288x960.py\",\n",
    "    checkpoint_file: str = \"raft_8x2_50k_kitti2015_288x960.pth\",\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    if model == None:\n",
    "        model = init_model(config_file, checkpoint_file, device=device)\n",
    "    result = inference_model(model, img1, img2)\n",
    "    img = res\n",
    "    vel = []\n",
    "    flow_map = None\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        mid_x = int(x1 + ((x2 - x1) / 2))\n",
    "        mid_y = int(y1 + ((y2 - y1) / 2))\n",
    "        flows_u = result[int(y1) : int(y2), int(x1) : int(x2), 0]\n",
    "        flows_v = result[int(y1) : int(y2), int(x1) : int(x2), 1]\n",
    "        mean_u = flows_u.mean()\n",
    "        mean_v = flows_v.mean()\n",
    "        img = draw_arrow(img, mean_u, mean_v, (mid_x, mid_y))\n",
    "        flow_map = visualize_flow(result, save_file=f\"{out_dir}/flow_map.png\")\n",
    "        vel.append(math.sqrt(pow(mean_u, 2) + (pow(mean_v, 2))))\n",
    "    if save_img:\n",
    "        cv2.imwrite(f\"{out_dir}/arrow_and_box.png\", img)\n",
    "    return vel, img, flow_map\n",
    "\n",
    "def infer_video(\n",
    "    video_path:str,\n",
    "    names:list = [\"person\", \"car\", \"airplane\"],\n",
    "    config_file: str = \"raft_8x2_50k_kitti2015_288x960.py\",\n",
    "    checkpoint_file: str = \"raft_8x2_50k_kitti2015_288x960.pth\",\n",
    "    device:str=\"cpu\",\n",
    "):    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame1 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"ERROR! In Reading the video file..\")\n",
    "        return\n",
    "    frames = []\n",
    "    yolo = load_obj_model(task=\"detect\")\n",
    "    model = init_model(config=config_file, checkpoint=checkpoint_file, device=device)\n",
    "    speeds = []\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while cap.isOpened():\n",
    "        ret, frame2 = cap.read()\n",
    "        if ret:\n",
    "            img, boxes, _, _ = get_points(yolo=yolo, names=names, img=frame2, annotate=True, return_img=True, stream=False)\n",
    "            speed, img, _ = get_velocity(img1=frame1, img2=frame2, boxes=boxes, res=img, model=model, save_img=False)\n",
    "            frames.append(img)\n",
    "            speeds.append(speed)\n",
    "        else:\n",
    "            break\n",
    "        frame1 = frame2\n",
    "    return frames, speeds, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def generate_video(\n",
    "        frames:list, \n",
    "        fps:int, \n",
    "        video_path:str\n",
    "    ):\n",
    "    out = cv2.VideoWriter(video_path,cv2.VideoWriter_fourcc(*'mp4v'), fps, (frames[0].shape[1] ,frames[0].shape[0]))\n",
    "    for i in range(len(frames)):\n",
    "        out.write(frames[i].astype(np.uint8))\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 6 persons, 6 cars, 2 handbags, 283.1ms\n",
      "Speed: 28.8ms preprocess, 283.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "img = cv2.imread(\"sample_1.jpg\")\n",
    "obj_model = load_obj_model(task=\"detect\")\n",
    "yolo_results = detect_objects(model=obj_model, task=\"detect\", image=img, return_only_boxes=False)\n",
    "annotated_image = annotateImage(image=img, results=yolo_results, draw_bbox=True, draw_mask=False, conf_thresh=0.5, area_thresh=200)\n",
    "cv2.imwrite(\"yolo_test.jpg\", annotated_image)\n",
    "\n",
    "# img = cv2.imread(\"sample_1.jpg\")\n",
    "# obj_model = load_obj_model(task=\"segment\")\n",
    "# sam_results = detect_objects(model=obj_model, task=\"segment\", image=img, return_only_boxes=False)\n",
    "# annotated_image = annotateImage(image=img, results=sam_results, draw_bbox=False, draw_mask=True, conf_thresh=0.5, area_thresh=200)\n",
    "# cv2.imwrite(\"sam_test.jpg\", annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def intersection_area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a[2], b[2]) - max(a[0], b[0])\n",
    "    dy = min(a[3], b[3]) - max(a[1], b[1])\n",
    "    # print(dx, dy)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "\n",
    "\n",
    "def detect_obstacles(img:np.ndarray,\n",
    "                     target_box:list, \n",
    "                     factor:float = 0.5,\n",
    "                     conf:float = 0.25, \n",
    "                     iou:float = 0.7, \n",
    "                     imgsz:int = 640,\n",
    "                     augment:bool = False,\n",
    "                     model = None, \n",
    "                     objects = [\"car\"], \n",
    "                     alpha:float = 0.4\n",
    "    ):\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    obstacles_list = []\n",
    "    if model:\n",
    "        # result = detect_objects(model=model, image=img, stream=True, task=\"detect\", return_only_boxes=False, names=objects)\n",
    "        result = detect_objects(model=model, task=\"detect\", image=img, return_only_boxes=False)\n",
    "    else:\n",
    "        result = detect_objects(model=load_obj_model(), image=img, stream=True, task=\"detect\", return_only_boxes=False, names=objects, conf=conf, iou=iou, augment=augment, imgsz=imgsz)\n",
    "    \n",
    "    target_area = (target_box[2]-target_box[0]) * (target_box[3]-target_box[1])\n",
    "    img = annotateImage(image=img, results=result, label=True)\n",
    "    i = 0\n",
    "    res_img = None\n",
    "    for box in result.xyxy:\n",
    "        \n",
    "        inter_area = intersection_area(box, target_box)\n",
    "        if inter_area:\n",
    "            ratio = inter_area / target_area\n",
    "            if ratio >= factor:\n",
    "                obstacles_list.append(result[i])\n",
    "                # img = cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color = (255, 0, 0), thickness=2)\n",
    "        i+=1\n",
    "    if len(obstacles_list) == 0:\n",
    "        res_img = cv2.rectangle(img.copy(), (int(target_box[0]), int(target_box[1])), (int(target_box[2]), int(target_box[3])), color = (0,255,0), thickness=2)\n",
    "    else:\n",
    "        res_img = cv2.rectangle(img.copy(), (int(target_box[0]), int(target_box[1])), (int(target_box[2]), int(target_box[3])), (255,0,0), 2)\n",
    "    # res_img = cv2.addWeighted(res_img, alpha, img, 1 - alpha, 0)\n",
    "    return obstacles_list, res_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def obstacle_avoidance(video_path:str, \n",
    "                       target_box:list, \n",
    "                       factor:float=0.01, \n",
    "                       objects:list = [\"airplane\", \"car\", \"person\", \"bus\", \"truck\"]\n",
    "    ):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    yolo = load_obj_model(task=\"detect\")\n",
    "\n",
    "    frames = []\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            obstacle_list, res_img = detect_obstacles(img=frame, target_box=target_box, factor=factor, model=yolo, objects=objects)\n",
    "            # plt.imshow(res_img)\n",
    "            res_img = cv2.cvtColor(res_img, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(res_img)\n",
    "            print(\"frame No:\", len(frames))\n",
    "        else:\n",
    "            break\n",
    "    return frames, fps, obstacle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# img = cv2.imread(filename=\"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/sample_1.jpg\")\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# # plt.imshow(img)\n",
    "# model = load_obj_model()\n",
    "# objs = detect_objects(model=model, image=img, return_only_boxes=False, names=[\"person\"])\n",
    "# # print(objs[0].xyxy[0])\n",
    "# obstacles, res_img = detect_obstacles(img=img, target_box=objs[0].xyxy[0], factor=0.1)\n",
    "# print(obstacles)\n",
    "# plt.imshow(res_img)\n",
    "\n",
    "\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/example_obs_det.mp4\"\n",
    "# target_box = [2000, 1600, 2250, 1700]\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/camera-104-2023-07-05_15-36-29.ts\"\n",
    "# target_box = [300, 20, 520, 320]\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/jbt_videos/camera-163-2023-07-05_22-12-55.ts\"\n",
    "# target_box = [250, 0, 448,  800]\n",
    "# (800, 448, 3)\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/jbt_videos/camera-159-2023-07-05_23-29-46.ts\"\n",
    "# target_box = [250, 0, 448, 800]\n",
    "\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/jbt_videos/camera-107-2023-07-14_01-50-52.ts\"\n",
    "# target_box = [250, 0, 448, 800]\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/jbt_videos/camera-107-2023-07-05_22-35-27.ts\"\n",
    "# target_box = [250, 0, 448, 800]\n",
    "\n",
    "video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/camera-102-2023-07-07_00-11-27.ts\"\n",
    "target_box = [150, 450, 300, 700]\n",
    "# video_path = \"/media/ali/A4D00431D0040BEC/ALI/Ali/dreamai_pocs/JBT/SAM/jbt_videos/camera-32-2023-07-12_11-35-06.ts\"\n",
    "# target_box = [0, 200, 512, 384]\n",
    "# (384, 512, 3)\n",
    "\n",
    "\n",
    "frames, fps, obstacle_list = obstacle_avoidance(video_path=video_path, target_box=target_box, factor=0.01)\n",
    "generate_video(video_path=\"example_det_obs.mp4\", frames=frames, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: raft_8x2_50k_kitti2015_288x960.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 96.3ms\n",
      "Speed: 2.3ms preprocess, 96.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.2ms\n",
      "Speed: 4.3ms preprocess, 116.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 100.8ms\n",
      "Speed: 2.1ms preprocess, 100.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 113.5ms\n",
      "Speed: 2.2ms preprocess, 113.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.9ms\n",
      "Speed: 2.8ms preprocess, 120.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 160.4ms\n",
      "Speed: 4.5ms preprocess, 160.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "frames, vels, fps = infer_video(video_path=\"camera-104-2023-07-05_15-34-29.ts\",names=[\"person\", \"car\", \"bus\", \"truck\", \"suitcase\", \"backpack\", \"handbag\", \"airplane\"])\n",
    "generate_video(frames=frames, fps=fps, video_path=\"output_arrow_hahah.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# Object Segmentation with SAM...\n",
    "\n",
    "\n",
    "def get_mask_area(mask: np.ndarray):\n",
    "    area = mask.sum()  # assumes binary mask (True == 1)\n",
    "    return area\n",
    "\n",
    "\n",
    "def calculateIoU(gtMask, predMask):\n",
    "    # Calculate the true positives,\n",
    "    # false positives, and false negatives\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(gtMask.shape[0]):\n",
    "        for j in range(gtMask.shape[1]):\n",
    "            if gtMask[i][j] == 1 and predMask[i][j] == 1:\n",
    "                tp += 1\n",
    "            elif gtMask[i][j] == 0 and predMask[i][j] == 1:\n",
    "                fp += 1\n",
    "            elif gtMask[i][j] == 1 and predMask[i][j] == 0:\n",
    "                fn += 1\n",
    "    # Calculate IoU\n",
    "    iou = tp / (tp + fp + fn)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def segment_with_prompts(sam_model: Sam, image: np.ndarray, **kwargs):\n",
    "    h, w, _ = image.shape\n",
    "    points = kwargs.get(\n",
    "        \"points\", np.array([[w * 0.5, h * 0.5], [0, h], [w, 0], [0, 0], [w, h]])\n",
    "    )\n",
    "    labels = kwargs.get(\"labels\", np.array([1, 0, 0, 0, 0]))\n",
    "    mask = kwargs.get(\"mask\", None)\n",
    "    if mask != None:\n",
    "      mask = st.resize(mask, (256, 256), order=0, preserve_range=True, anti_aliasing=False)\n",
    "      mask = np.stack((mask,) * 1, axis=0)\n",
    "    predictor = SamPredictor(sam_model)\n",
    "    predictor.set_image(image)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=points, point_labels=labels, mask_input=mask, multimask_output=False\n",
    "    )\n",
    "    return masks\n",
    "\n",
    "\n",
    "def load_sam_model(\n",
    "    sam_checkpoint: str = \"sam_vit_h_4b8939.pth\",\n",
    "    model_type: str = \"vit_h\",\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=device)\n",
    "    return sam\n",
    "\n",
    "def segment_everything(sam_model:Sam, image:np.ndarray, **kwargs):\n",
    "    mask = kwargs[\"mask\"]\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
    "    masks = mask_generator.generate(image)\n",
    "    if mask == None:\n",
    "        return masks\n",
    "    sorted_anns = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "    best = -1.0\n",
    "    ind = -100\n",
    "\n",
    "    area1 = get_mask_area(mask.astype(int))\n",
    "    for i in range(10):\n",
    "        val = calculateIoU(mask.astype(int), sorted_anns[i]['segmentation'].astype(int))\n",
    "        area2 = get_mask_area(sorted_anns[i]['segmentation'].astype(int))\n",
    "        dif = abs(area2 - area1)\n",
    "        if val > best and dif < 5000:\n",
    "            ind = i\n",
    "            best = val\n",
    "        elif val > best:\n",
    "            ind = i\n",
    "            best = val\n",
    "    return sorted_anns[ind]\n",
    "\n",
    "def segment(sam_model:Sam, image:np.ndarray, seg_function=segment_with_prompts, **kwargs):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  mask_fname = kwargs.get(\"mask_path\", None)\n",
    "  mask = None\n",
    "  if mask_fname != None:\n",
    "    mask = cv2.imread(mask_fname)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
    "    mask = mask.astype(bool)\n",
    "  h,w,_ = image.shape\n",
    "  points = kwargs.get(\n",
    "        \"points\", np.array([[w * 0.5, h * 0.5], [0, h], [w, 0], [0, 0], [w, h]])\n",
    "  )\n",
    "  labels = kwargs.get(\"labels\", np.array([1, 0, 0, 0, 0]))\n",
    "\n",
    "  masks = seg_function(sam_model, image, mask=mask, points=points, labels=labels)\n",
    "  return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "def get_points(\n",
    "    yolo, \n",
    "    img: Union[str, np.ndarray],\n",
    "    draw_bbox: bool = False,\n",
    "    return_img: bool = False,\n",
    "    stream: bool = True,\n",
    "):\n",
    "    result = detect_objects(model=yolo, img=img, stream=stream, draw_bbox=draw_bbox)\n",
    "    points = []\n",
    "    labels = []\n",
    "    for box in result[0][\"boxes\"]:\n",
    "        x1, y1, x2, y2 = box[:4]\n",
    "        mid_x = int(x1 + ((x2 - x1) / 2))\n",
    "        mid_y = int(y1 + ((y2 - y1) / 2))\n",
    "        points.append([mid_x, mid_y])\n",
    "        labels.append(1)  #\n",
    "    if return_img:\n",
    "        return result[0][\"img\"], result[0][\"boxes\"], points, labels\n",
    "    return result[0][\"boxes\"], points, labels\n",
    "\n",
    "\n",
    "# Extract direction and speed from the selected objects using RAFT (optical flow algorithm)..\n",
    "\n",
    "\n",
    "def display_direction(result: np.ndarray, mean_u, mean_v, points):\n",
    "    h_rat = 10\n",
    "    w_rat = 10\n",
    "    image_arr = cv2.arrowedLine(\n",
    "        img=result,\n",
    "        pt1=(points[0], points[1]),\n",
    "        pt2=(points[0] + int(mean_u) * w_rat, points[1] + int(mean_v) * h_rat),\n",
    "        color=(0, 0, 255),\n",
    "        thickness=5,\n",
    "        line_type=8,\n",
    "        tipLength=0.5,\n",
    "    )\n",
    "    return image_arr\n",
    "\n",
    "\n",
    "def get_velocity(\n",
    "    img1: Union[str, np.ndarray],\n",
    "    img2: Union[str,np.ndarray],\n",
    "    boxes: list,\n",
    "    res: np.ndarray,\n",
    "    model=None,\n",
    "    save_img: bool = True,\n",
    "    out_dir: str = \"./frames/\",\n",
    "    config_file: str = \"raft_8x2_50k_kitti2015_288x960.py\",\n",
    "    checkpoint_file: str = \"raft_8x2_50k_kitti2015_288x960.pth\",\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    if model == None:\n",
    "        model = init_model(config_file, checkpoint_file, device=device)\n",
    "    result = inference_model(model, img1, img2)\n",
    "    img = res\n",
    "    vel = 0\n",
    "    flow_map = None\n",
    "    # print(boxes)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box[:4]\n",
    "        mid_x = int(x1 + ((x2 - x1) / 2))\n",
    "        mid_y = int(y1 + ((y2 - y1) / 2))\n",
    "        flows_u = result[int(y1) : int(y2), int(x1) : int(x2), 0]\n",
    "        flows_v = result[int(y1) : int(y2), int(x1) : int(x2), 1]\n",
    "        mean_u = flows_u.mean()\n",
    "        mean_v = flows_v.mean()\n",
    "        img = display_direction(img, mean_u, mean_v, (mid_x, mid_y))\n",
    "        flow_map = visualize_flow(result, save_file=\"flow_map.png\")\n",
    "        vel = math.sqrt(pow(mean_u, 2) + (pow(mean_v, 2)))\n",
    "    if save_img:\n",
    "        cv2.imwrite(\"arrow_and_box.png\", img)\n",
    "    return vel, img, flow_map\n",
    "\n",
    "def infer_video(video_path:str,\n",
    "    names:list = [\"person\", \"car\", \"airplane\"],\n",
    "    config_file: str = \"raft_8x2_50k_kitti2015_288x960.py\",\n",
    "    checkpoint_file: str = \"raft_8x2_50k_kitti2015_288x960.pth\",\n",
    "    device:str=\"cpu\",\n",
    "):    \n",
    "    print(names)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame1 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"ERROR! In Reading the video file..\")\n",
    "        return\n",
    "    frames = []\n",
    "    yolo = load_obj_model()\n",
    "    model = init_model(config=config_file, checkpoint=checkpoint_file, device=device)\n",
    "    speeds = []\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    i = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame2 = cap.read()\n",
    "        if ret:\n",
    "            # frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            img, boxes, _, _ = get_points(yolo=yolo, names=names,img=frame2, draw_bbox=True, return_img=True)\n",
    "            speed, img, _ = get_velocity(img1=frame1, img2=frame2, boxes=boxes, res=img, model=model, save_img=False)\n",
    "            frames.append(img)\n",
    "            speeds.append(speed)\n",
    "        else:\n",
    "            break\n",
    "        i+=1\n",
    "        frame1 = frame2\n",
    "    return frames, speeds, fps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "frames, vels, fps = infer_video(video_path=\"/home/trillo3/projects/vid_chains/static_files/104/archive/2023/07/05/camera-104-2023-07-05_15-34-29.ts\",names=[\"person\", \"car\", \"bus\", \"truck\", \"suitcase\", \"backpack\", \"handbag\", \"airplane\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "def load_obj_model(name=\"yolov8n.pt\"):\n",
    "    return YOLO(name)\n",
    "\n",
    "\n",
    "def detect_objects(model, img, names=None, exclude=None,  stream=True, draw_bbox=False, return_only_boxes=True):\n",
    "    dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
    "    classes = list(dict.keys())\n",
    "    if names:\n",
    "        classes = [list(dict.keys())[list(dict.values()).index(name.lower())] for name in names]\n",
    "    if exclude:\n",
    "        exclude = [e.lower() for e in exclude]\n",
    "        names = [v for v in list(dict.values()) if not v in exclude]\n",
    "        classes = [list(dict.keys())[list(dict.values()).index(name)] for name in names]\n",
    "    res = model(img, stream=stream, classes=classes)\n",
    "    # if draw_bbox:\n",
    "        # return [{\"boxes\": r.boxes.data.detach().cpu().tolist(),\"img\": r.plot()} for r in res]\n",
    "    if return_only_boxes:\n",
    "        return [{\"boxes\": r.boxes.data.detach().cpu().tolist()} for r in res]\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones(\n",
    "        (sorted_anns[0][\"segmentation\"].shape[0], sorted_anns[0][\"segmentation\"].shape[1], 4)\n",
    "    )\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann[\"segmentation\"]\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "def combine_mask(image:np.ndarray, mask:np.ndarray, color:tuple=None):\n",
    "    if color == None:\n",
    "        color = (30, 144, 255)\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h,w)\n",
    "    image[mask_image] = color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# with segment_everything..\n",
    "\n",
    "# frame_dir = 'frames'\n",
    "# frames = os.listdir(frame_dir)\n",
    "# # print(frames)\n",
    "# for frame in frames[:5]:\n",
    "#   image = cv2.imread(f'{frame_dir}/{frame}')\n",
    "#   sam = load_sam_model()\n",
    "#   mask_3 = segment(sam_model=sam, image=image, seg_function = segment_everything)\n",
    "#   plt.figure(figsize=(10,10))\n",
    "#   plt.imshow(image)\n",
    "#   show_mask(mask_3, plt.gca())\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# Clone the repository:\n",
    "# !git clone https://github.com/gaomingqi/Track-Anything.git\n",
    "# %cd /content/Track-Anything\n",
    "\n",
    "# Install dependencies:\n",
    "# !pip install -r requirements.txt\n",
    "# new libraries: progressbar2 gdown gitpython openmim av hickle tqdm psutil gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# Object tracking with TAN..\n",
    "\n",
    "# download checkpoints\n",
    "def download_checkpoint(url, folder, filename):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"download checkpoints ......\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(\"download successfully!\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "def download_checkpoint_from_google_drive(file_id, folder, filename):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Downloading checkpoints from Google Drive... tips: If you cannot see the progress bar, please try to download it manuall \\\n",
    "              and put it in the checkpointes directory. E2FGVI-HQ-CVPR22.pth: https://github.com/MCG-NKU/E2FGVI(E2FGVI-HQ model)\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, filepath, quiet=False)\n",
    "        print(\"Downloaded successfully!\")\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# generate video after vos inference\n",
    "def generate_video_from_frames(frames:list, output_path:str, fps:int=30):\n",
    "    \"\"\"\n",
    "    Generates a video from a list of frames.\n",
    "\n",
    "    Args:\n",
    "        frames (list of numpy arrays): The frames to include in the video.\n",
    "        output_path (str): The path to save the generated video.\n",
    "        fps (int, optional): The frame rate of the output video. Defaults to 30.\n",
    "    \"\"\"\n",
    "    # height, width, layers = frames[0].shape\n",
    "    # fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    # video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    # print(output_path)\n",
    "    # for frame in frames:\n",
    "    #     video.write(frame)\n",
    "\n",
    "    # video.release()\n",
    "    frames = torch.from_numpy(np.asarray(frames))\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n",
    "    return output_path\n",
    "\n",
    "def generate_frames_from_video(video_path:str, start_time:int):\n",
    "  frames = []\n",
    "  try:\n",
    "      cap = cv2.VideoCapture(video_path)\n",
    "      cap.set(cv2.CAP_PROP_POS_MSEC, start_time*1000)\n",
    "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "      while cap.isOpened():\n",
    "          ret, frame = cap.read()\n",
    "          if ret == True:\n",
    "              frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "          else:\n",
    "              break\n",
    "  except (OSError, TypeError, ValueError, KeyError, SyntaxError) as e:\n",
    "      print(\"read_frame_source:{} error. {}\\n\".format(video_path, str(e)))\n",
    "  return frames\n",
    "\n",
    "def track_object(images:list, points:np.ndarray, labels:np.ndarray, e2fgvi_checkpoint:str, sam_checkpoint:str, xmem_checkpoint:str, **kwargs):\n",
    "  sys.argv = [\"cuda:0\"]\n",
    "  args = parse_augment()\n",
    "  multimask = kwargs.get('multimask', True)\n",
    "  track_model = TrackingAnything(sam_checkpoint, xmem_checkpoint, e2fgvi_checkpoint, args)\n",
    "  track_model.samcontroler.sam_controler.reset_image()\n",
    "  track_model.samcontroler.sam_controler.set_image(images[0])\n",
    "  mask,_,_ = track_model.first_frame_click(image = images[0], points = points, labels = labels, multimask = multimask)\n",
    "  masks, logits ,painted_images= track_model.generator(images, mask)\n",
    "  return masks, logits, painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# check and download checkpoints if needed\n",
    "SAM_checkpoint_dict = {\n",
    "    'vit_h': \"sam_vit_h_4b8939.pth\",\n",
    "    'vit_l': \"sam_vit_l_0b3195.pth\",\n",
    "    \"vit_b\": \"sam_vit_b_01ec64.pth\"\n",
    "}\n",
    "SAM_checkpoint_url_dict = {\n",
    "    'vit_h': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
    "    'vit_l': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
    "    'vit_b': \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "}\n",
    "sam_checkpoint = SAM_checkpoint_dict['vit_h']\n",
    "sam_checkpoint_url = SAM_checkpoint_url_dict['vit_h']\n",
    "xmem_checkpoint = \"XMem-s012.pth\"\n",
    "xmem_checkpoint_url = \"https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem-s012.pth\"\n",
    "e2fgvi_checkpoint = \"E2FGVI-HQ-CVPR22.pth\"\n",
    "e2fgvi_checkpoint_id = \"10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3\"\n",
    "\n",
    "folder = \"./checkpoints\"\n",
    "sam_checkpoint = download_checkpoint(sam_checkpoint_url, folder, sam_checkpoint)\n",
    "xmem_checkpoint = download_checkpoint(xmem_checkpoint_url, folder, xmem_checkpoint)\n",
    "e2fgvi_checkpoint = download_checkpoint_from_google_drive(e2fgvi_checkpoint_id, folder, e2fgvi_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# extract frames from the video...\n",
    "frames = generate_frames_from_video('vid_shorts.mp4', start_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "h, w, _ = frames[0].shape\n",
    "points=np.array([[int(w*0.5), int(h*0.5)], [0, h-10], [w-10, 0], [0,0], [w-10,h-10]])\n",
    "labels = np.array([1, 0, 0, 0, 0])\n",
    "# Track the masked object using point prompt..\n",
    "masks, logits, painted_images = track_object(frames, points = points, labels = labels, e2fgvi_checkpoint = e2fgvi_checkpoint, sam_checkpoint = sam_checkpoint, xmem_checkpoint = xmem_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "# Save the return frames in the form of a video..\n",
    "output_path = 'output.mp4'\n",
    "output_path = generate_video_from_frames(frames=frames, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "img_path = \"../imgs/\"\n",
    "\n",
    "model = load_obj_model()\n",
    "objects = detect_objects(model, img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | eval: false\n",
    "\n",
    "\n",
    "objects[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
