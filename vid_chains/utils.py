# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_utils.ipynb.

# %% auto 0
__all__ = ['load_obj_model', 'detect_objects', 'annotateImage', 'get_width', 'list_widths', 'centroid', 'list_centroids',
           'inter_dist', 'focal_len_to_px', 'camera_to_obj_dist', 'get_points', 'draw_arrow', 'get_velocity',
           'infer_video', 'generate_video', 'intersection_area', 'detect_obstacles', 'obstacle_avoidance']

# %% ../nbs/00_utils.ipynb 2
from .imports import *

# %% ../nbs/00_utils.ipynb 4
def load_obj_model(
        yolo_name:str = "yolov8n.pt",
        sam_name:str = "sam_b.pt", 
        task:str = "detect"
    ):
    if task == "detect":
        return YOLO(yolo_name)
    elif task == "segment":
        return SAM(sam_name)
    print(f"Model does not exist for the following task: {task}. Please select one of the following tasks: detect or segment")
    return

def detect_objects(
        model,
        image:Union[np.ndarray, str],
        stream:bool = True,
        task:str = "detect",
        conf:float = 0.25,
        iou:float = 0.7,
        augment:bool = False,
        imgsz:int = 640, 
        names:list = None, 
        exclude:list = None,   
        return_only_boxes:bool = True, 
        points:list = None, 
        labels:list = None, 
        bboxes:list = None
    ):
    results = None
    if task == "detect": # Using the yolo model
        classes_dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}
        classes = list(classes_dict.keys())
        if names:
            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name.lower())] for name in names]
        if exclude:
            exclude = [e.lower() for e in exclude]
            names = [v for v in list(classes_dict.values()) if not v in exclude]
            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name)] for name in names]
        # print(classes)
        results = model(image, stream=stream, classes=classes, conf=conf, iou=iou, augment=augment, imgsz=imgsz)
        if return_only_boxes:
            return [{"boxes": r.boxes.data.detach().cpu().tolist()} for r in results]
    elif task == "segment": # Using the SAM model..
        results = model(image, stream = stream, bboxes = bboxes, points = points, labels = labels)
    detections = None
    if results:
        for result in results:
            detections = sv.Detections.from_ultralytics(result)
        return detections
    print(f"Can not process the following task: {task}. Please select one of the following tasks: detect or segment")
    return

    
def annotateImage(
        image:np.ndarray, 
        results, 
        draw_bbox:bool = True,
        draw_mask:bool = False, 
        label:bool = True,
        conf_thresh:float = 0.0, 
        names:list = None, 
        exclude:list = None, 
        area_thresh:int = 0
    ):
    labels = None
    detections=results
    if label:
        classes_dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}
        classes = list(classes_dict.keys())
        if names:
            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name.lower())] for name in names]
        if exclude:
            exclude = [e.lower() for e in exclude]
            names = [v for v in list(classes_dict.values()) if not v in exclude]
            classes = [list(classes_dict.keys())[list(classes_dict.values()).index(name)] for name in names]
        detections = detections[detections.confidence > conf_thresh]
        detections = detections[np.isin(detections.class_id, classes)]
        labels = [
            f"{classes_dict[class_id]} {confidence:.2f}"
            for class_id, confidence
            in zip(detections.class_id, detections.confidence)
        ]
    detections = detections[detections.area > area_thresh]
    annotated_image = image.copy()
    if draw_bbox:
        box_annotator = sv.BoxAnnotator()
        annotated_image = box_annotator.annotate(annotated_image, detections, labels=labels)
    if draw_mask:
        mask_annotator = sv.MaskAnnotator()
        annotated_image = mask_annotator.annotate(scene=annotated_image, detections=detections)
    return annotated_image

def get_width(l):
    w = l[2] - l[0]
    return w


def list_widths(obj):
    w = []
    for i in range(0, len(obj.get("boxes"))):
        l = []
        for j in range(0, 6):
            l.append(obj.get("boxes")[i][j])
        if (
            l[5] == 39.0
        ):  # very specific test case for bottles so will ignore other objects, will remove this in the future
            width = get_width(l)
            w.append(width)
    return w


def centroid(l):
    t = []
    cx = (l[0] + l[2]) / 2.0
    cy = (l[1] + l[3]) / 2.0
    t.append(cx)
    t.append(cy)
    return t


def list_centroids(obj):
    c = []
    for i in range(0, len(obj.get("boxes"))):
        l = []
        for j in range(0, 4):
            l.append(obj.get("boxes")[i][j])
        centre = centroid(l)
        c.append(centre)
    return c


def inter_dist(obj):
    c = list_centroids(obj)
    dis = []
    st = []
    for i in range(0, len(c)):
        for j in range(i + 1, len(c)):
            # st.append("Distance b/w object "+str(i)+" and object "+str(j))
            # st.append("D("+str(i)+","+str(j)+")")
            dis.append(math.dist(c[i], c[j]))
    # return st,dis
    return dis


def focal_len_to_px(focal_len, sensor_px):
    return round((focal_len / sensor_px) * 1000)


def camera_to_obj_dist(focal_length_px, obj, real_width):
    widths = list_widths(obj)
    dists = []
    for w in widths:
        distance = (real_width * focal_length_px) / w
        dists.append(distance)

    return dists

# %% ../nbs/00_utils.ipynb 5
# Extract direction and speed from the selected objects using RAFT (optical flow algorithm)..


def get_points(
    yolo,
    names:list,
    img: Union[str, np.ndarray],
    annotate: bool = False,
    return_img: bool = False,
    stream: bool = True,
):
    detections = detect_objects(model=yolo, image=img, stream=stream, task="detect", names=names, return_only_boxes=False)
    points = []
    labels = []
    boxes = detections.xyxy
    for box in boxes:
        x1, y1, x2, y2 = box
        mid_x = int(x1 + ((x2 - x1) / 2))
        mid_y = int(y1 + ((y2 - y1) / 2))
        points.append([mid_x, mid_y])
        labels.append(1)  #
    if annotate:
        annotated_image = annotateImage(image=img, results=detections, draw_bbox=True, draw_mask=False)
    if return_img:
        return annotated_image, boxes, points, labels
    return boxes, points, labels

def draw_arrow(img: np.ndarray, mean_u, mean_v, points):
    h_rat = 10
    w_rat = 10
    image_arr = cv2.arrowedLine(
        img=img,
        pt1=(points[0], points[1]),
        pt2=(points[0] + int(mean_u) * w_rat, points[1] + int(mean_v) * h_rat),
        color=(0, 0, 255),
        thickness=5,
        line_type=8,
        tipLength=0.5,
    )
    return image_arr


def get_velocity(
    img1: Union[str, np.ndarray],
    img2: Union[str,np.ndarray],
    boxes: list,
    res: np.ndarray,
    model = None,
    save_img: bool = True,
    out_dir: str = "./frames/",
    config_file: str = "raft_8x2_50k_kitti2015_288x960.py",
    checkpoint_file: str = "raft_8x2_50k_kitti2015_288x960.pth",
    device: str = "cpu",
):
    if model == None:
        model = init_model(config_file, checkpoint_file, device=device)
    result = inference_model(model, img1, img2)
    img = res
    vel = []
    flow_map = None
    for box in boxes:
        x1, y1, x2, y2 = box
        mid_x = int(x1 + ((x2 - x1) / 2))
        mid_y = int(y1 + ((y2 - y1) / 2))
        flows_u = result[int(y1) : int(y2), int(x1) : int(x2), 0]
        flows_v = result[int(y1) : int(y2), int(x1) : int(x2), 1]
        mean_u = flows_u.mean()
        mean_v = flows_v.mean()
        img = draw_arrow(img, mean_u, mean_v, (mid_x, mid_y))
        flow_map = visualize_flow(result, save_file=f"{out_dir}/flow_map.png")
        vel.append(math.sqrt(pow(mean_u, 2) + (pow(mean_v, 2))))
    if save_img:
        cv2.imwrite(f"{out_dir}/arrow_and_box.png", img)
    return vel, img, flow_map

def infer_video(
    video_path:str,
    names:list = ["person", "car", "airplane"],
    config_file: str = "raft_8x2_50k_kitti2015_288x960.py",
    checkpoint_file: str = "raft_8x2_50k_kitti2015_288x960.pth",
    device:str="cpu",
):    
    cap = cv2.VideoCapture(video_path)
    ret, frame1 = cap.read()
    if not ret:
        print("ERROR! In Reading the video file..")
        return
    frames = []
    yolo = load_obj_model(task="detect")
    model = init_model(config=config_file, checkpoint=checkpoint_file, device=device)
    speeds = []
    fps = cap.get(cv2.CAP_PROP_FPS)
    while cap.isOpened():
        ret, frame2 = cap.read()
        if ret:
            img, boxes, _, _ = get_points(yolo=yolo, names=names, img=frame2, annotate=True, return_img=True, stream=False)
            speed, img, _ = get_velocity(img1=frame1, img2=frame2, boxes=boxes, res=img, model=model, save_img=False)
            frames.append(img)
            speeds.append(speed)
        else:
            break
        frame1 = frame2
    return frames, speeds, fps

# %% ../nbs/00_utils.ipynb 6
def generate_video(
        frames:list, 
        fps:int, 
        video_path:str
    ):
    out = cv2.VideoWriter(video_path,cv2.VideoWriter_fourcc(*'mp4v'), fps, (frames[0].shape[1] ,frames[0].shape[0]))
    for i in range(len(frames)):
        out.write(frames[i].astype(np.uint8))
    out.release()

# %% ../nbs/00_utils.ipynb 8
def intersection_area(a, b):  # returns None if rectangles don't intersect
    dx = min(a[2], b[2]) - max(a[0], b[0])
    dy = min(a[3], b[3]) - max(a[1], b[1])
    # print(dx, dy)
    if (dx>=0) and (dy>=0):
        return dx*dy


def detect_obstacles(img:np.ndarray,
                     target_box:list, 
                     factor:float = 0.5,
                     conf:float = 0.25, 
                     iou:float = 0.7, 
                     imgsz:int = 640,
                     augment:bool = False,
                     model = None, 
                     objects = ["car"], 
                     alpha:float = 0.4
    ):
    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    obstacles_list = []
    if model:
        # result = detect_objects(model=model, image=img, stream=True, task="detect", return_only_boxes=False, names=objects)
        result = detect_objects(model=model, task="detect", image=img, return_only_boxes=False)
    else:
        result = detect_objects(model=load_obj_model(), image=img, stream=True, task="detect", return_only_boxes=False, names=objects, conf=conf, iou=iou, augment=augment, imgsz=imgsz)
    
    target_area = (target_box[2]-target_box[0]) * (target_box[3]-target_box[1])
    img = annotateImage(image=img, results=result, label=True)
    i = 0
    res_img = None
    for box in result.xyxy:
        
        inter_area = intersection_area(box, target_box)
        if inter_area:
            ratio = inter_area / target_area
            if ratio >= factor:
                obstacles_list.append(result[i])
                # img = cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color = (255, 0, 0), thickness=2)
        i+=1
    if len(obstacles_list) == 0:
        res_img = cv2.rectangle(img.copy(), (int(target_box[0]), int(target_box[1])), (int(target_box[2]), int(target_box[3])), color = (0,255,0), thickness=2)
    else:
        res_img = cv2.rectangle(img.copy(), (int(target_box[0]), int(target_box[1])), (int(target_box[2]), int(target_box[3])), (255,0,0), 2)
    # res_img = cv2.addWeighted(res_img, alpha, img, 1 - alpha, 0)
    return obstacles_list, res_img



# %% ../nbs/00_utils.ipynb 9
def obstacle_avoidance(video_path:str, 
                       target_box:list, 
                       factor:float=0.01, 
                       objects:list = ["airplane", "car", "person", "bus", "truck"]
    ):
    cap = cv2.VideoCapture(video_path)
    yolo = load_obj_model(task="detect")

    frames = []
    fps = cap.get(cv2.CAP_PROP_FPS)
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
            obstacle_list, res_img = detect_obstacles(img=frame, target_box=target_box, factor=factor, model=yolo, objects=objects)
            # plt.imshow(res_img)
            res_img = cv2.cvtColor(res_img, cv2.COLOR_BGR2RGB)
            frames.append(res_img)
            print("frame No:", len(frames))
        else:
            break
    return frames, fps, obstacle_list
